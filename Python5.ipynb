{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db695dd",
   "metadata": {},
   "source": [
    "# Spark SQL and Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb0e0a",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c55350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .enableHiveSupport()\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5d9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql.types import LongType\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    " return s * s * s\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325272e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca051e45",
   "metadata": {},
   "source": [
    "### Speeding up and distributing PySpark UDFs with Pandas UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9394a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import various pyspark SQL functions including pandas_udf\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "# Declare the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    " return a * a * a\n",
    "# Create the pandas UDF for the cubed function\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be2a535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a Pandas Series\n",
    "x = pd.Series([1, 2, 3])\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33cba4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.range(1, 4)\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad981ce",
   "metadata": {},
   "source": [
    "# Querying with the Spark SQL Shell, Beeline, and Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02332d2",
   "metadata": {},
   "source": [
    "# Higher-Order Functions in DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac12b9",
   "metadata": {},
   "source": [
    "## Higher-Order Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8046c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70258f",
   "metadata": {},
   "source": [
    "#### transform()\n",
    "transform(array<T>, function<T, U>): array<U>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8e179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    " transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea6ee3",
   "metadata": {},
   "source": [
    "#### filter()\n",
    "filter(array<T>, function<T, Boolean>): array<T>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f855591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    " filter(celsius, t -> t > 38) as high\n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ffcbc",
   "metadata": {},
   "source": [
    "#### exists()\n",
    "exists(array<T>, function<T, V, Boolean>): Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9378b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    " exists(celsius, t -> t = 38) as threshold\n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a95ed8",
   "metadata": {},
   "source": [
    "#### reduce()\n",
    "reduce(array<T>, B, function<B, T, B>, function<B, R>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95626b04",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rw-rw-rw-",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SARA~1.ARR\\AppData\\Local\\Temp/ipykernel_3204/2367486860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m## Calculate average temperature and convert to F\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m spark.sql(\"\"\"\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mSELECT\u001b[0m \u001b[0mcelsius\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m  reduce(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rw-rw-rw-"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "## Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    " reduce(\n",
    " celsius,\n",
    " 0,\n",
    " (t, acc) -> t + acc,\n",
    " acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    " ) as avgFahrenheit\n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14a871",
   "metadata": {},
   "source": [
    "# Common DataFrames and Spark SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6653d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "from pyspark.sql.functions import expr\n",
    "tripdelaysFilePath = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/flights/departuredelays.csv\"\n",
    "airportsnaFilePath = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/flights/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain airports data set\n",
    "airportsna = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    " .load(airportsnaFilePath))\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\")\n",
    " .load(tripdelaysFilePath))\n",
    "departureDelays = (departureDelays\n",
    " .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    " .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and\n",
    " date like '01010%' and delay > 0\"\"\")))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe940eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ffece87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfee37a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840daeb0",
   "metadata": {},
   "source": [
    "## Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb465020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union two tables\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395450e",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1639fc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(airportsna, airportsna.IATA == foo.origin).select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493735c",
   "metadata": {},
   "source": [
    "## Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d479355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a47dd",
   "metadata": {},
   "source": [
    "### Adding new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c319e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "foo2 = (foo.withColumn(\n",
    " \"status\",\n",
    " expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    " ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9968a1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b9d1d",
   "metadata": {},
   "source": [
    "### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b53cd21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05e59b",
   "metadata": {},
   "source": [
    "### Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57bd26d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff02362",
   "metadata": {},
   "source": [
    "### Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574eae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
