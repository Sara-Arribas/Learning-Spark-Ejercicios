{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0205d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "# Create a DataFrame using SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"AuthorsAges\")\n",
    " .getOrCreate())\n",
    "# Create a DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03db2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    " (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "# Group the same names together, aggregate their ages, and compute an average\n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c9f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"author\", StringType(), False),\n",
    "StructField(\"title\", StringType(), False),\n",
    "StructField(\"pages\", IntegerType(), False)])\n",
    "\n",
    "## Con DDL \n",
    "# schema = \"author STRING, title STRING, pages INT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf919be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'countDistinct'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SARA~1.ARR\\AppData\\Local\\Temp/ipykernel_1196/3808541489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountDistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1657\u001b[0m         \"\"\"\n\u001b[0;32m   1658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'countDistinct'"
     ]
    }
   ],
   "source": [
    "data_df.countDistinct(\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39b5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# In Python \n",
    "from pyspark.sql import SparkSession\n",
    "# Define schema for our data using DDL \n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\n",
    "\n",
    "\"LinkedIn\"]],\n",
    " [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    " [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "[\"twitter\", \"FB\"]],\n",
    " [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "[\"twitter\", \"LinkedIn\"]]\n",
    " ]\n",
    "# Main program\n",
    "# if __name__ == \"__main__\":\n",
    " # Create a SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"Example-3_6\")\n",
    " .getOrCreate())\n",
    " # Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    " # Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    " # Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc15979",
   "metadata": {},
   "source": [
    "## Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edba0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    " [\"twitter\", \"LinkedIn\"])\n",
    "# access using index for individual items\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df1b5b",
   "metadata": {},
   "source": [
    "## Using DataFrameReader and DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0813dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, define a schema\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    " StructField('UnitID', StringType(), True),\n",
    " StructField('IncidentNumber', IntegerType(), True),\n",
    " StructField('CallType', StringType(), True),\n",
    " StructField('CallDate', StringType(), True),\n",
    " StructField('WatchDate', StringType(), True),\n",
    " StructField('CallFinalDisposition', StringType(), True),\n",
    " StructField('AvailableDtTm', StringType(), True),\n",
    " StructField('Address', StringType(), True),\n",
    " StructField('City', StringType(), True),\n",
    " StructField('Zipcode', IntegerType(), True),\n",
    " StructField('Battalion', StringType(), True),\n",
    " StructField('StationArea', StringType(), True),\n",
    " StructField('Box', StringType(), True),\n",
    " StructField('OriginalPriority', StringType(), True),\n",
    " StructField('Priority', StringType(), True),\n",
    " StructField('FinalPriority', IntegerType(), True),\n",
    " StructField('ALSUnit', BooleanType(), True),\n",
    " StructField('CallTypeGroup', StringType(), True),\n",
    " StructField('NumAlarms', IntegerType(), True),\n",
    " StructField('UnitType', StringType(), True),\n",
    " StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    " StructField('FirePreventionDistrict', StringType(), True),\n",
    " StructField('SupervisorDistrict', StringType(), True),\n",
    " StructField('Neighborhood', StringType(), True),\n",
    " StructField('Location', StringType(), True),\n",
    " StructField('RowID', StringType(), True),\n",
    " StructField('Delay', FloatType(), True)])\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da2828",
   "metadata": {},
   "source": [
    "## Saving a DataFrame as a Parquet file or SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c24fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/parquet/fairDF.parquet\"\n",
    "fire_df.repartition(1).write.mode(\"overwrite\").format(\"parquet\").save(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052be0c",
   "metadata": {},
   "source": [
    "## Projections and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_fire_df = (fire_df\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde3024b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.sql.functions.count_distinct does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SARA~1.ARR\\AppData\\Local\\Temp/ipykernel_1196/2208808289.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m (fire_df.select(\"CallType\")\n\u001b[0;32m      4\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CallType\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m  \u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountDistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CallType\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DistinctCallTypes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m  .show())\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\functions.py\u001b[0m in \u001b[0;36mcountDistinct\u001b[1;34m(col, *cols)\u001b[0m\n\u001b[0;32m    989\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m     \"\"\"\n\u001b[1;32m--> 991\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcount_distinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\functions.py\u001b[0m in \u001b[0;36mcount_distinct\u001b[1;34m(col, *cols)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m     \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_distinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1533\u001b[0m                     answer, self._gateway_client, self._fqn, name)\n\u001b[0;32m   1534\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m             raise Py4JError(\n\u001b[0m\u001b[0;32m   1536\u001b[0m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001b[0;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JError\u001b[0m: org.apache.spark.sql.functions.count_distinct does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "# return number of distinct types of calls using countDistinct()\n",
    "from pyspark.sql.functions import *\n",
    "(fire_df.select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec6d491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "(fire_df.select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cedfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, filter for only distinct non-null CallTypes from all the rows\n",
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .distinct()\n",
    " .show(10, False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7335ff8",
   "metadata": {},
   "source": [
    "## Renaming, adding, and dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0e3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where(col(\"ResponseDelayedinMins\") > 5)\n",
    " .show(5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31875ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    " fire_df.select(\"Delay\").where(col(\"Delay\") > 5).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8447f",
   "metadata": {},
   "source": [
    "1. Convert the existing columnâ€™s data type from string to a Spark-supported timestamp.\n",
    "\n",
    "2. Use the new format specified in the format string \"MM/dd/yyyy\" or \"MM/dd/yyyy hh:mm:ss a\" where appropriate.\n",
    "\n",
    "3. After converting to the new data type, drop() the old column and append the new one specified in the first argument to the withColumn() method.\n",
    "\n",
    "4. Assign the new modified DataFrame to fire_ts_df.\n",
    "\n",
    "The queries result in three new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea271d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df = (new_fire_df\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\"))\n",
    "# Select the converted columns\n",
    "(fire_ts_df\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c421374",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fire_ts_df\n",
    " .select(year('IncidentDate'))\n",
    " .distinct()\n",
    " .orderBy(year('IncidentDate'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b508664",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fire_ts_df\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    " .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650ded7",
   "metadata": {},
   "source": [
    "## Other common DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5149aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "(fire_ts_df\n",
    " .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),\n",
    " F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb96918",
   "metadata": {},
   "source": [
    "## End-to-End DataFrame Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eeb2af",
   "metadata": {},
   "source": [
    "**1) How many distinct types of calls were made to the Fire Department?**\n",
    "**2) What are distinct types of calls were made to the Fire Department?**\n",
    "**3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c2a5736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df.select(\"CallType\").where(col(\"CallType\").isNotNull()).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103053ae",
   "metadata": {},
   "source": [
    "**2) What are distinct types of calls were made to the Fire Department?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c36b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.select(\"CallType\").distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566b65a",
   "metadata": {},
   "source": [
    "**3) Find out all response or delayed times greater than 5 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93d6f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "|4.983333             |\n",
      "+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"ResponseDelayedinMins\").where(col(\"ResponseDelayedinMins\")<5).orderBy(desc(\"ResponseDelayedinMins\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82138d32",
   "metadata": {},
   "source": [
    "**4) What are the most common call types?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5417214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"CallType\").groupBy(\"CallType\").count().orderBy(desc(\"count\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ce868",
   "metadata": {},
   "source": [
    "**5) What San Francisco neighborhoods are in zip codes 94102 and 94103**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "539a0331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------+\n",
      "|Neighborhood                  |ZipCode|\n",
      "+------------------------------+-------+\n",
      "|Potrero Hill                  |94103  |\n",
      "|Western Addition              |94102  |\n",
      "|Tenderloin                    |94102  |\n",
      "|Nob Hill                      |94102  |\n",
      "|Castro/Upper Market           |94103  |\n",
      "|South of Market               |94102  |\n",
      "|South of Market               |94103  |\n",
      "|Hayes Valley                  |94103  |\n",
      "|Financial District/South Beach|94102  |\n",
      "|Mission Bay                   |94103  |\n",
      "+------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"Neighborhood\",\"ZipCode\").where((col(\"ZipCode\") == 94102) | (col(\"ZipCode\") == 94103)).distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e04e44",
   "metadata": {},
   "source": [
    "**6) How many distinct years of data is in the CSV file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21210650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|2000              |\n",
      "|2001              |\n",
      "|2002              |\n",
      "|2003              |\n",
      "|2004              |\n",
      "|2005              |\n",
      "|2006              |\n",
      "|2007              |\n",
      "|2008              |\n",
      "|2009              |\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(year(\"IncidentDate\")).distinct().orderBy(year(\"IncidentDate\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc7745",
   "metadata": {},
   "source": [
    "**7) What week of the year in 2018 had the most fire calls?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "525f3211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|weekofyear(IncidentDate)|count|\n",
      "+------------------------+-----+\n",
      "|22                      |259  |\n",
      "|40                      |255  |\n",
      "|43                      |250  |\n",
      "|25                      |249  |\n",
      "|1                       |246  |\n",
      "|44                      |244  |\n",
      "|32                      |243  |\n",
      "|13                      |243  |\n",
      "|11                      |240  |\n",
      "|5                       |236  |\n",
      "+------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.filter(year(\"IncidentDate\") == 2018).groupBy(weekofyear(\"IncidentDate\")).count().orderBy(desc(\"count\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32ed31",
   "metadata": {},
   "source": [
    "## Typed Objects, Untyped Objects, and Generic Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37533ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, \"Learning Spark 2E\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30976f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecaebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
