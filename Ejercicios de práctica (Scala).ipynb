{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c17787c",
   "metadata": {},
   "source": [
    "## Ejemplo M&M's Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "/**\n",
    " * Usage: MnMcount <mnm_file_dataset>\n",
    " */\n",
    "// object MnMcount {\n",
    "// def main(args: Array[String]) {\n",
    "// A través de un SparkSession se puede crear un DataFrame o se puede registrar un DataFrame como una tabla.\n",
    " val spark = SparkSession \n",
    " .builder\n",
    " .appName(\"MnMCount\")\n",
    " .getOrCreate()\n",
    " //if (args.length < 1) {\n",
    "// print(\"Usage: MnMcount <mnm_file_dataset>\")\n",
    " //sys.exit(1)\n",
    " //}\n",
    " // Get the M&M data set filename\n",
    " //val mnmFile = args(0)\n",
    " // Read the file into a Spark DataFrame\n",
    " val mnmDF = spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\") // más util para \n",
    " .load(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/mnm_dataset.csv\")\n",
    " // Aggregate counts of all colors and groupBy() State and Color\n",
    " // orderBy() in descending order\n",
    " val countMnMDF = mnmDF\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(count(\"Count\").alias(\"Total\"))\n",
    " .orderBy(desc(\"Total\"))\n",
    " // Show the resulting aggregations for all the states and colors\n",
    " countMnMDF.show(60)\n",
    " println(s\"Total Rows = ${countMnMDF.count()}\")\n",
    " println()\n",
    " // Find the aggregate counts for California by filtering\n",
    " val caCountMnMDF = mnmDF\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .where(col(\"State\") === \"CA\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(count(\"Count\").alias(\"Total\"))\n",
    " .orderBy(desc(\"Total\"))\n",
    " // Show the resulting aggregations for California\n",
    " caCountMnMDF.show(10)\n",
    " // Stop the SparkSession\n",
    " spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession\n",
    "// Create a DataFrame using SparkSession\n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"AuthorsAges\")\n",
    " .getOrCreate()\n",
    "// Create a DataFrame of names and ages\n",
    "val dataDF = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Brooke\", 25),\n",
    " (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")\n",
    "// Group the same names together, aggregate their ages, and compute an average\n",
    "val avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))\n",
    "// Show the results of the final execution\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57444877",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb9c9b",
   "metadata": {},
   "source": [
    " ## Ejemplo de creación de schema y DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befead4",
   "metadata": {},
   "source": [
    "Antes de ponernos a escribir la tabla, definimos la estructura de la misma para ahorrar trabajo a Spark y facilitarnos el trabajo a la hora de corregir errores al haber definido ya los tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schema = StructType(Array(StructField(\"author\", StringType, false),\n",
    " StructField(\"title\", StringType, false),\n",
    " StructField(\"pages\", IntegerType, false)))\n",
    "\n",
    "// lo mismo pero usando DDL (Data Definition Language) parece mucho más simple:\n",
    "// val schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    " val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"Example-3_7\")\n",
    " .getOrCreate()\n",
    "/* if (args.length <= 0) {\n",
    " println(\"usage Example3_7 <file path to blogs.json>\")\n",
    " System.exit(1)\n",
    " }\n",
    "  */\n",
    " // Get the path to the JSON file\n",
    " val jsonFile = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/blogs.json\"\n",
    "\n",
    " // Define our schema programmatically\n",
    " val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    " StructField(\"First\", StringType, false),\n",
    " StructField(\"Last\", StringType, false),\n",
    " StructField(\"Url\", StringType, false),\n",
    " StructField(\"Published\", StringType, false),\n",
    " StructField(\"Hits\", IntegerType, false),\n",
    " StructField(\"Campaigns\", ArrayType(StringType), false)))\n",
    " // Create a DataFrame by reading from the JSON file \n",
    " // with a predefined schema\n",
    " val blogsDF = spark.read.schema(schema).json(jsonFile)\n",
    " // Show the DataFrame schema as output\n",
    " blogsDF.show(false)\n",
    "\n",
    " // Print the schema\n",
    " println(blogsDF.printSchema)\n",
    " println(blogsDF.schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943a7b4",
   "metadata": {},
   "source": [
    "## Columns and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "blogsDF.columns\n",
    "\n",
    "// Access a particular column with col and it returns a Column type\n",
    "blogsDF.col(\"Id\")\n",
    "\n",
    "// Use an expression to compute a value\n",
    "blogsDF.select(expr(\"Hits * 2\")).show(2)\n",
    "// or use col to compute value\n",
    "blogsDF.select(col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use an expression to compute big hitters for blogs\n",
    "// This adds a new column, Big Hitters, based on the conditional expression\n",
    "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Concatenate three columns, create a new column, and show the\n",
    "// newly created concatenated column\n",
    "blogsDF\n",
    " .withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\"))))\n",
    " .select(col(\"AuthorsId\"))\n",
    " .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// These statements return the same value, showing that\n",
    "// expr is the same as a col method call\n",
    "blogsDF.select(expr(\"Hits\")).show(2)\n",
    "blogsDF.select(col(\"Hits\")).show(2)\n",
    "blogsDF.select(\"Hits\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sort by column \"Id\" in descending order\n",
    "blogsDF.sort(col(\"Id\").desc).show()\n",
    "blogsDF.sort($\"Id\".desc).show()\n",
    "\n",
    "// $ before the name of the column is a function in Spark that converts column named Id to a Column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a268e",
   "metadata": {},
   "source": [
    "## Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05581d3",
   "metadata": {},
   "source": [
    "A row in Spark is a generic Row object, containing one or more columns. Each col‐\n",
    "umn may be of the same data type (e.g., integer or string), or they can have different\n",
    "types (integer, string, map, array, etc.). Because Row is an object in Spark and an\n",
    "ordered collection of fields, you can instantiate a Row in each of Spark’s supported lan‐\n",
    "guages and access its fields by an index starting at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ca429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "// Create a Row\n",
    "val blogRow = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    " Array(\"twitter\", \"LinkedIn\"))\n",
    "// Access using index for individual items\n",
    "blogRow(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07215a9",
   "metadata": {},
   "source": [
    "Row objects can be used to create DataFrames if you need them for quick interactivity\n",
    "and exploration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\"))\n",
    "val authorsDF = rows.toDF(\"Author\", \"State\")\n",
    "authorsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2740c",
   "metadata": {},
   "source": [
    "## Using DataFrameReader and DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5feace",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "                                  StructField(\"UnitID\", StringType, true),\n",
    "                                  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "                                  StructField(\"CallType\", StringType, true),\n",
    "                                  StructField(\"CallDate\", StringType, true),\n",
    "                                  StructField(\"WatchDate\", StringType, true),\n",
    "                                  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "                                  StructField(\"AvailableDtTm\", StringType, true),\n",
    "                                  StructField(\"Address\", StringType, true),\n",
    "                                  StructField(\"City\", StringType, true),\n",
    "                                  StructField(\"Zipcode\", IntegerType, true),\n",
    "                                  StructField(\"Battalion\", StringType, true),\n",
    "                                  StructField(\"StationArea\", StringType, true),\n",
    "                                  StructField(\"Box\", StringType, true),\n",
    "                                  StructField(\"OriginalPriority\", StringType, true),\n",
    "                                  StructField(\"Priority\", StringType, true),\n",
    "                                  StructField(\"FinalPriority\", IntegerType, true),\n",
    "                                  StructField(\"ALSUnit\", BooleanType, true),\n",
    "                                  StructField(\"CallTypeGroup\", StringType, true),\n",
    "                                  StructField(\"NumAlarms\", IntegerType, true),\n",
    "                                  StructField(\"UnitType\", StringType, true),\n",
    "                                  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "                                  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "                                  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "                                  StructField(\"Neighborhood\", StringType, true),\n",
    "                                  StructField(\"Location\", StringType, true),\n",
    "                                  StructField(\"RowID\", StringType, true),\n",
    "                                  StructField(\"Delay\", FloatType, true)))\n",
    "// Read the file using the CSV DataFrameReader\n",
    "val sfFireFile = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/sf-fire-calls.csv\"\n",
    "val fireDF = spark.read.schema(fireSchema)\n",
    " .option(\"header\", \"true\")\n",
    " .csv(sfFireFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379df4e9",
   "metadata": {},
   "source": [
    "## Saving a DataFrame as a Parquet file or SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "// In Scala to save as a Parquet file\n",
    "val parquetPath = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/parquet/fire_df2.parquet\"\n",
    "fireDF.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquetTable = \"fireTable\" // name of the table\n",
    "fireDF.write.format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1f552",
   "metadata": {},
   "source": [
    "## Projections and filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad514e82",
   "metadata": {},
   "source": [
    "A projection in relational parlance is a way to return only the rows matching a certain relational condition by using filters. In Spark, projections are done with the select() method, while filters can be expressed using the filter() or\n",
    "where() method. We can use this technique to examine specific aspects of our SF Fire Department data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15decf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fewFireDF = fireDF\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where($\"CallType\" =!= \"Medical Incident\")\n",
    "fewFireDF.show(5, false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca837e",
   "metadata": {},
   "source": [
    "What if we want to know how many distinct CallTypes were recorded as the causes\n",
    "of the fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c209a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "fireDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull)\n",
    " .agg(countDistinct('CallType) as 'DistinctCallTypes)\n",
    " .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85698ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF\n",
    " .select(\"CallType\")\n",
    " .where($\"CallType\".isNotNull)\n",
    " .distinct()\n",
    " .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ea693",
   "metadata": {},
   "source": [
    "## Renaming, adding, and dropping columns\n",
    "\n",
    "You could selectively rename columns with the withColumnRenamed() method. For instance, let’s change the name of our Delay column to ResponseDelayedinMins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where($\"ResponseDelayedinMins\" > 5)\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfb94f",
   "metadata": {},
   "source": [
    "1. Convert the existing column’s data type from string to a Spark-supported timestamp.\n",
    "\n",
    "2. Use the new format specified in the format string \"MM/dd/yyyy\" or \"MM/dd/yyyy hh:mm:ss a\" where appropriate.\n",
    "\n",
    "3. After converting to the new data type, drop() the old column and append the new one specified in the first argument to the withColumn() method.\n",
    "\n",
    "4. Assign the new modified DataFrame to fire_ts_df.\n",
    "\n",
    "The queries result in three new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fireTsDF = newFireDF\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\")\n",
    "// Select the converted columns\n",
    "fireTsDF\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb47c54",
   "metadata": {},
   "source": [
    "Now that we have modified the dates, we can query using functions from\n",
    "spark.sql.functions like month(), year(), and day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF\n",
    " .select(year($\"IncidentDate\"))\n",
    " .distinct()\n",
    " .orderBy(year($\"IncidentDate\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f3e75",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ca29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull)\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(desc(\"count\"))\n",
    " .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf47c4",
   "metadata": {},
   "source": [
    "## Other common DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbde37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => F}\n",
    "fireTsDF\n",
    " .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),\n",
    " F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665caaeb",
   "metadata": {},
   "source": [
    "For more advanced statistical needs common with data science workloads, read the API documentation for methods like stat(), describe(), correlation(), covariance(), sampleBy(), approxQuantile(), frequentItems(), and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e4a55",
   "metadata": {},
   "source": [
    "## End-to-End DataFrame Example : Fire Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96869b39",
   "metadata": {},
   "source": [
    "**1) What were all the different types of fire calls in 2018?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.select(\"CallType\").where(col(\"CallType\").isNotNull).distinct().show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.select(\"CallType\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26364649",
   "metadata": {},
   "source": [
    "**2) What months within the year 2018 saw the highest number of fire calls?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => F}\n",
    "fireTsDF.filter(year($\"IncidentDate\") === 2018).groupBy(month($\"IncidentDate\")).count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5568e",
   "metadata": {},
   "source": [
    "**3) Which neighborhood in San Francisco generated the most fire calls in 2018?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897093dc",
   "metadata": {},
   "source": [
    "Este código me saca todos los vecindarios agrupados por el número de llamadas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73673a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF.filter(year($\"IncidentDate\") === 2018).groupBy(\"Neighborhood\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea1cf0",
   "metadata": {},
   "source": [
    "Para quedarme sólo con una bastaría con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF.filter(year($\"IncidentDate\") === 2018).groupBy(\"Neighborhood\").count().orderBy(desc(\"count\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c092b6",
   "metadata": {},
   "source": [
    "**Which neighborhoods had the worst response times to fire calls in 2018?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF.select(\"Neighborhood\",\"ResponseDelayedinMins\").filter(year($\"IncidentDate\") === 2018)\n",
    ".orderBy(desc(\"ResponseDelayedinMins\")).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90819811",
   "metadata": {},
   "source": [
    "**Is there a correlation between neighborhood, zip code, and number of fire calls?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF.select(\"Neighborhood\",\"ZipCode\").groupBy(\"Neighborhood\",\"ZipCode\").count()\n",
    ".orderBy(desc(\"count\")).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b32672",
   "metadata": {},
   "source": [
    "## Typed Objects, Untyped Objects, and Generic Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0111df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "val row = Row(350, true, \"Learning Spark 2E\", null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc61e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "row.getInt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c705f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "row.getBoolean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "row.getString(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30055716",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "case class DeviceIoTData (battery_level: Long, c02_level: Long,\n",
    "cca2: String, cca3: String, cn: String, device_id: Long,\n",
    "device_name: String, humidity: Long, ip: String, latitude: Double,\n",
    "lcd: String, longitude: Double, scale:String, temp: Long,\n",
    "timestamp: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = spark.read\n",
    ".json(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark\")\n",
    ".as[DeviceIoTData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8793a45",
   "metadata": {},
   "source": [
    "## Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val filterTempDS = ds.filter(ds(\"temp\") > 30 && ds(\"humidity\") > 70)\n",
    "filterTempDS.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long,\n",
    " cca3: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsTemp = ds.filter(ds(\"temp\") > 25)\n",
    " .select(\"temp\",\"device_name\",\"device_id\",\"cca3\")\n",
    " .toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\")\n",
    " .as[DeviceTempByCountry]\n",
    "dsTemp.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1916e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "val device = dsTemp.first()\n",
    "println(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3328de",
   "metadata": {},
   "source": [
    "## End-to-End Dataset Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e183d7",
   "metadata": {},
   "source": [
    "**1). Detect failing devices with battery levels below a threshold.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5203d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.select($\"battery_level\",$\"c02_level\",$\"device_name\").where($\"battery_level\" < 8).sort($\"c02_level\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5006e5f",
   "metadata": {},
   "source": [
    "**2. Identify offending countries with high levels of CO2 emissions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44333ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val newDS = ds.select(\"cn\",\"c02_level\").filter(ds(\"c02_level\")>1300).groupBy($\"cn\").avg().sort($\"avg(c02_level)\".desc)\n",
    "newDS.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259127d3",
   "metadata": {},
   "source": [
    "**3. Compute the min and max values for temperature, battery level, CO2, and humidity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.select(min(\"temp\"),max(\"temp\"),min(\"humidity\"),max(\"humidity\"),min(\"c02_level\"),max(\"c02_level\"),min(\"battery_level\"), \n",
    "          max(\"battery_level\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb134b5",
   "metadata": {},
   "source": [
    "**4. Sort and group by average temperature, CO2, humidity, and country**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca070a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.select(\"temp\", \"c02_level\", \"humidity\", \"cn\").groupBy($\"cn\").avg()\n",
    ".sort($\"avg(temp)\".desc,$\"avg(c02_level)\".desc).as(\"avg(humidity)\").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
