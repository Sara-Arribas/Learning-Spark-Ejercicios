{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0636c85",
   "metadata": {},
   "source": [
    "## Basic Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db31a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .enableHiveSupport()\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    " .getOrCreate())\n",
    "# Path to data set\n",
    "csv_file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a9ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"`date` STRING, `delay` INT, `distance` INT,`origin` STRING, `destination` STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f9bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b44d3d",
   "metadata": {},
   "source": [
    "**Es todo exactamente igual que en scala**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cadac",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(col(\"distance\") > 1000)\n",
    " .orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f7c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(\"distance > 1000\")\n",
    " .orderBy(\"distance\", ascending=False).show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9a8d8",
   "metadata": {},
   "source": [
    "#### Exercise: \n",
    "Convert the date column into a readable format and find the days or months when these delays were most common. Were the delays related to winter months or holidays?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509025a",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "try converting the other two SQL queries to use the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5598c60",
   "metadata": {},
   "source": [
    "## SQL Tables and Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542ac39",
   "metadata": {},
   "source": [
    "### Creating SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a2115d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0aab35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e991862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to our US flight delays CSV file\n",
    "csv_file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/departuredelays.csv\"\n",
    "# Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566280a",
   "metadata": {},
   "source": [
    "### Creating Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1957e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6787dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In SQL\n",
    "# SELECT * FROM us_origin_airport_JFK_tmp_view\n",
    "# In Scala/Python\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "## Or\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1188c580",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edaecb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/parquet/2010-summary.parquet/\"\n",
    "df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c319b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d56080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64dfb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36cce2b",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8067e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file =\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/json/*\"\n",
    "df = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d722403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de637b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_13556/457288978.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\SARA~1.ARR\\AppData\\Local\\Temp/ipykernel_13556/457288978.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    (df.write.json(\"/tmp/data/json/df_json3\")\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "(df.write.json(\"/tmp/data/json/df_json3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89c116",
   "metadata": {},
   "source": [
    "## CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb422a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file =\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/csv/*\"\n",
    "schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "df = (spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .schema(schema)\n",
    " .option(\"mode\", \"FAILFAST\") # Exit if any errors\n",
    " .option(\"nullValue\", \"\") # Replace any null data field with quotes\n",
    " .load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d474c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ce353c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6c38c",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02b5900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"avro\").load(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/avro/*\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a80e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    " .format(\"avro\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"/tmp/data/avro/df_avro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d1a4d",
   "metadata": {},
   "source": [
    "## ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6754535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/orc/*\"\n",
    "df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5f5d719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e06336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"orc\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/orc/flights_orc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e89102e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import image\n",
    "image_dir = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/train_images/\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcf7dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+-----+\n",
      "|height|width|nChannels|mode|label|\n",
      "+------+-----+---------+----+-----+\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |1    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "+------+-----+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    " \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec27f0",
   "metadata": {},
   "source": [
    "## Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6569b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|                path|    modificationTime|length|             content|label|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 55037|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54634|[FF D8 FF E0 00 1...|    1|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54624|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54505|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54475|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+--------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/train_images/\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e64eaa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|                path|    modificationTime|length|             content|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 55037|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54634|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54624|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54505|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/sa...|2021-12-23 09:53:...| 54475|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .option(\"recursiveFileLookup\", \"true\")\n",
    " .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc5bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d071a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
