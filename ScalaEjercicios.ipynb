{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fef440",
   "metadata": {},
   "source": [
    "# Capítulo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbb7b0",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "Descargar el Quijote https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79. Aplicar no solo count (para obtener el número de líneas) y show sino probar distintas sobrecargas del método show (con/sin truncate, indicando/sin indicar num de filas, etc) así como también los métodos, head, take, first (diferencias entre estos 3?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f3fa0",
   "metadata": {},
   "source": [
    "### Inicio SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50823210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://L2111011.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1640613445028)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.functions._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5cec1ca2\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession \n",
    " .builder\n",
    " .appName(\"Ejercicios\")\n",
    " .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166b86b",
   "metadata": {},
   "source": [
    "### Leo fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ca84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val quijote = spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(\"C:/Users/sara.arribas/Documents/Big Data/Ejercicios/el_quijote.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54177e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb60fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd229f4",
   "metadata": {},
   "source": [
    "**Diferencia**: \n",
    "- head() coge la cabecera\n",
    "- take(n) coge las n primeras líneas\n",
    "- first() coge la primera línea (si hay cabecera la cabecera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a29e2",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "**Del ejercicio de M&M aplicar:**\n",
    "\n",
    "**a) Otras operaciones de agregación como el Max con otro tipo de ordenamiento (descendiente).**\n",
    "\n",
    "**b) hacer un ejercicio como el “where” de CA que aparece en el libro pero indicando más opciones de estados (p.e. NV, TX, CA, CO).**\n",
    "\n",
    "**c) Hacer un ejercicio donde se calculen en una misma operación el Max, Min, Avg, Count. Revisar el API (documentación) donde encontrarán este ejemplo:**\n",
    "\n",
    "**ds.agg(max($\"age\"), avg($\"salary\"))**\n",
    "**ds.groupBy().agg(max($\"age\"), avg($\"salary\"))**\n",
    "\n",
    "\n",
    "**d) Hacer también ejercicios en SQL creando tmpView**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    " val mnmDF = spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\") // más util para \n",
    " .load(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/mnm_dataset.csv\")\n",
    " // Aggregate counts of all colors and groupBy() State and Color\n",
    " // orderBy() in descending order\n",
    " val maxMnMDF = mnmDF\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(max(\"Count\").alias(\"Maximo\"))\n",
    " .orderBy(desc(\"Maximo\"))\n",
    " maxMnMDF.show()\n",
    "\n",
    " // Find the aggregate counts for California by filtering\n",
    " val caCountMnMDF = mnmDF\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .where(col(\"State\") === \"CA\" || col(\"State\") === \"NV\" || col(\"State\") === \"TX\" || col(\"State\") === \"CO\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(count(\"Count\").alias(\"Total\"))\n",
    " .orderBy(desc(\"Total\"))\n",
    " // Show the resulting aggregations for California\n",
    " caCountMnMDF.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295cd56",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7e01c",
   "metadata": {},
   "source": [
    "# Capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e773b0",
   "metadata": {},
   "source": [
    "**a) Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3510df",
   "metadata": {},
   "outputs": [],
   "source": [
    " val mnmDF = spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\") // más util para \n",
    " .load(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/mnm_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnmDF.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8724e6",
   "metadata": {},
   "source": [
    "**b) Cuando se define un schema al definir un campo por ejemplo StructField('Delay',FloatType(), True) ¿qué significa el último parámetro Boolean?**\n",
    "\n",
    "Significa si admite Null como parámetro.\n",
    "\n",
    "**c). Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?**\n",
    "\n",
    "Un dataframe es un dataset que al mismo tiempo está organizado también en columnas. Cada columna tiene su nombre correspondiente.\n",
    "\n",
    "**d) Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y guardar los datos en los formatos: JSON, CSV (dándole otro nombre para evitar sobrescribir el fichero origen), AVRO**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb25e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquetPath = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/parquet/mnm_dataset.parquet\"\n",
    "mnmDF.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write.json(\"/tmp/data/json/mnm_DF_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72cf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/mnm_DF_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5576da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write.format(\"avro\").mode(\"overwrite\").save(\"/tmp/data/avro/mnm_DF_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773afab",
   "metadata": {},
   "source": [
    "**f). Revisar al guardar los ficheros (p.e. json, csv, etc) el número de ficheros creados, revisar su contenido para comprender (constatar) como se guardan.**\n",
    "\n",
    "Se guardan por particiones\n",
    "\n",
    "**i) ¿A qué se debe que hayan más de un fichero?**\n",
    "\n",
    "Porque se crean particiones al guardar el archivo\n",
    "\n",
    "**ii) ¿Cómo obtener el número de particiones de un DataFrame?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71574b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.rdd.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ae29f",
   "metadata": {},
   "source": [
    "**iii)  ¿Qué formas existen para modificar el número de particiones de un DataFrame?**\n",
    "\n",
    "**iv) Llevar a cabo el ejemplo modificando el número de particiones a 1 y revisar de nuevo el/los ficheros guardados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.coalesce(4).write.format(\"json\").mode(\"overwrite\").save(\"/tmp/data/json/mnm_DF1_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f97111",
   "metadata": {},
   "source": [
    "En este caso al ser el archivo muy pequeño no pueden hacerse varias particiones pero el método es con coalesce(n) siendo n el número de particiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d68af",
   "metadata": {},
   "source": [
    "# Capítulo 4\n",
    "\n",
    "**a) Realizar todos los ejercicios propuestos de libro** \n",
    "\n",
    "Hecho\n",
    "\n",
    "**b) GlobalTempView vs TempView**\n",
    "\n",
    "Una TempView está sujetaa a una sóla SparkSession, mientras que la GlobalTempView estará disponible siempre.\n",
    "\n",
    "**c) Leer los AVRO, Parquet, JSON y CSV escritos en el cap3**\n",
    "\n",
    "Hecho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04974aef",
   "metadata": {},
   "source": [
    "# Capítulo 5\n",
    "\n",
    "**a) Realizar todos los ejercicios propuestos de libro (excepto los de hive en caso de utilizar spark instalado en local y en el caso de RDBMS hacer únicamente ejemplo especificado más adelante)**\n",
    "\n",
    "**b) Pros y Cons utilizar UDFs**\n",
    "\n",
    "**c) Instalar MySQL, descargar driver y cargar datos de BBDD de empleados https://dev.mysql.com/doc/employee/en/**\n",
    "\n",
    "i. Cargar con spark datos de empleados y departamentos\n",
    "\n",
    "ii. Mediante Joins mostrar toda la información de los empleados además de su título y salario.\n",
    "iii. Diferencia entre Rank y dense_rank (operaciones de ventana)\n",
    "iv. Utilizando operaciones de ventana obtener el salario, posición (cargo)\n",
    "y departamento actual de cada empleado, es decir, el último o más\n",
    "reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35d4902",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.sql.SQLSyntaxErrorException",
     "evalue": " Table 'employees.sql' doesn't exist\r",
     "output_type": "error",
     "traceback": [
      "java.sql.SQLSyntaxErrorException: Table 'employees.sql' doesn't exist\r",
      "  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\r",
      "  at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r",
      "  at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r",
      "  at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\r",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r",
      "  at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\r",
      "  ... 39 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "val employeesDF = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"employees.sql\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"root\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b3946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
