{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6c4587",
   "metadata": {},
   "source": [
    "## Basic Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae229303",
   "metadata": {},
   "outputs": [],
   "source": [
    "//package spark.hive\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.hive.HiveSessionStateBuilder\n",
    "import org.apache.spark.{SparkConf}\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "\n",
    "sparkConfig.set(\"spark.broadcast.compress\", \"false\")\n",
    "sparkConfig.set(\"spark.shuffle.compress\", \"false\")\n",
    "sparkConfig.set(\"spark.shuffle.spill.compress\", \"false\")\n",
    "sparkConfig.set(\"spark.io.compression.codec\", \"lzf\")\n",
    "sparkConfig.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "sparkConfig.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\")\n",
    "sparkConfig.set(\"spark.default.parallelism\",\"1\")\n",
    "sparkConfig.set(\"spark.shuffle.partitions\",\"1\")\n",
    "sparkConfig.set(\"spark.sql.hive.llap\", \"true\")\n",
    "sparkConfig.set(\"spark.datasource.hive.warehouse.load.staging.dir\",\"/tmp\")\n",
    "sparkConfig.set(\" spark.hadoop.metastore.catalog.default\",\"hive\")\n",
    "\n",
    "val _spark:SparkSession = SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"Unit Test\")\n",
    "  .config(sparkConfig)\n",
    "  .enableHiveSupport()\n",
    "  .getOrCreate()\n",
    "//val conf = new SparkConf\n",
    "   // .set(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/Hive\")\n",
    "     //  .set(\"spark.sql.catalogImplementation\",\"Hive\")\n",
    "     //  .setMaster(\"local[*]\")\n",
    "      // .setAppName(\"Hive Example\")\n",
    "//val spark = SparkSession\n",
    "  //.set(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/Hive\")\n",
    "  //.set(\"spark.sql.catalogImplementation\",\"Hive\")\n",
    "//  .setMaster(\"local[*]\")\n",
    " // .setAppName(\"Hive Example\")\n",
    " //.builder\n",
    " //.config(conf)\n",
    "// .enableHiveSupport()\n",
    "// .appName(\"SparkSQLExampleApp\")\n",
    "// .getOrCreate()\n",
    "// Path to data set\n",
    "val csvFile=\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/departuredelays.csv\"\n",
    "// Read and create a temporary view\n",
    "// Infer schema (note that for larger files you may want to specify the schema)\n",
    "val df = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csvFile)\n",
    "// Create a temporary view\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad502f",
   "metadata": {},
   "source": [
    "If you want to specify a schema, you can use a DDL-formatted string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985eba6",
   "metadata": {},
   "source": [
    "We’ll find all flights whose distance is greater than 1,000 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63285b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802dc29",
   "metadata": {},
   "source": [
    "As the results show, all of the longest flights were between Honolulu (HNL) and New York (JFK). Next, we’ll find all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239978ba",
   "metadata": {},
   "source": [
    "**Exercise**. Convert the date column into a readable format and find the days or months when these delays were most common. Were the delays related to winter months or holidays?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246ca7c",
   "metadata": {},
   "source": [
    "In the following example, we want to label all US flights, regardless of origin and destination, with an indication of the delays they experienced: Very Long Delays (> 6 hours), Long Delays (2–6 hours), etc. We’ll add these human-readable labels in a new column called Flight_Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a21348",
   "metadata": {},
   "source": [
    "**Exercise:** try converting the other two SQL queries to use the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54298205",
   "metadata": {},
   "source": [
    "## SQL Tables and Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b69c68",
   "metadata": {},
   "source": [
    "### Managed Versus UnmanagedTables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe79526",
   "metadata": {},
   "source": [
    "For a managed table, Spark manages both the metadata and the data in the file store. This could be a local filesystem, HDFS, or an object store such as Amazon S3 or Azure Blob. For an unmanaged table, Spark only manages the metadata, while you manage the data yourself in an external data source such as Cassandra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9319f",
   "metadata": {},
   "source": [
    "### Creating SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a20d148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe0740",
   "metadata": {},
   "source": [
    "#### Creating a managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25adc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.hive.HiveSessionStateBuilder\r\n",
       "import org.apache.spark.SparkConf\r\n",
       "sparkConfig: org.apache.spark.SparkConf = org.apache.spark.SparkConf@1ce27fe1\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@70afa2ff\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.hive.HiveSessionStateBuilder\n",
    "import org.apache.spark.{SparkConf}\n",
    "\n",
    "val sparkConfig = new SparkConf()\n",
    "\n",
    "sparkConfig.set(\"spark.broadcast.compress\", \"false\")\n",
    "sparkConfig.set(\"spark.shuffle.compress\", \"false\")\n",
    "sparkConfig.set(\"spark.shuffle.spill.compress\", \"false\")\n",
    "sparkConfig.set(\"spark.io.compression.codec\", \"lzf\")\n",
    "sparkConfig.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "sparkConfig.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\")\n",
    "sparkConfig.set(\"spark.default.parallelism\",\"1\")\n",
    "sparkConfig.set(\"spark.shuffle.partitions\",\"1\")\n",
    "sparkConfig.set(\"spark.sql.hive.llap\", \"true\")\n",
    "sparkConfig.set(\"spark.datasource.hive.warehouse.load.staging.dir\",\"/tmp\")\n",
    "sparkConfig.set(\" spark.hadoop.metastore.catalog.default\",\"hive\")\n",
    "\n",
    "val spark:SparkSession = SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"Unit Test\")\n",
    "  .config(sparkConfig)\n",
    "  .enableHiveSupport()\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7581f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c68f3228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c1f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|       default|\n",
      "|       flights|\n",
      "|learn_spark_db|\n",
      "+--------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val csv_file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/departuredelays.csv\"\n",
    "//Schema as defined in the preceding example\n",
    "val schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "val flights_df = spark.read.csv(csv_file, schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc50424",
   "metadata": {},
   "source": [
    "#### Creating an unmanaged table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27022b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n",
    " distance INT, origin STRING, destination STRING)\n",
    " USING csv OPTIONS (PATH\n",
    " 'C:/Users/sara.arribas/Downloads/Ejemplos_Spark/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566b12b",
   "metadata": {},
   "source": [
    "### Creating Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e12f9",
   "metadata": {},
   "source": [
    "Está igual en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "\n",
    "val df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65172190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55abad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a24aa7",
   "metadata": {},
   "source": [
    "## Viewing the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3439c",
   "metadata": {},
   "source": [
    "## Reading Tables into DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7dbd01",
   "metadata": {},
   "source": [
    "Let’s assume you have an existing database, learn_spark_db, and table, us_delay_flights_tbl, ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94391993",
   "metadata": {},
   "outputs": [],
   "source": [
    "val usFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "val usFlightsDF2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "usFlightsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071b41a",
   "metadata": {},
   "source": [
    "## DataFrameReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88f8de",
   "metadata": {},
   "source": [
    "It has a defined format and a recommended pattern for usage:\n",
    "\n",
    "DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ce3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use Parquet\n",
    "val file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/parquet/2010-summary.parquet\"\n",
    "//file.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcf6dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"parquet\").option(\"path\",file).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "val df2 = spark.read.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use CSV\n",
    "val df3 = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"mode\", \"PERMISSIVE\")\n",
    " .load(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/csv/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use JSON\n",
    "val df4 = spark.read.format(\"json\")\n",
    " .load(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/json/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c59809",
   "metadata": {},
   "source": [
    "### DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ejemplo\n",
    "//val location = ...\n",
    "//df.write.format(\"json\").mode(\"overwrite\").save(location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167f9f9",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8157b8d",
   "metadata": {},
   "source": [
    "### Reading Parquet files into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/parquet/2010-summary.parquet/\"\n",
    "val df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a2a7d",
   "metadata": {},
   "source": [
    "### Reading Parquet files into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47046047",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364357b",
   "metadata": {},
   "source": [
    "### Writing DataFrames to Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad545f",
   "metadata": {},
   "source": [
    "### Writing DataFrames to Spark SQL tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b537137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04458d32",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ebe241",
   "metadata": {},
   "source": [
    "### Reading a JSON file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/json/*\"\n",
    "val df = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3164a5d",
   "metadata": {},
   "source": [
    "### Reading a JSON file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb206a",
   "metadata": {},
   "source": [
    "### Writing DataFrames to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73119cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(\"/tmp/data/json/df_json2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95541a2f",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e259fb",
   "metadata": {},
   "source": [
    "### Reading a CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/csv/*\"\n",
    "val schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "val df = spark.read.format(\"csv\")\n",
    " .schema(schema)\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"mode\", \"FAILFAST\") // Exit if any errors\n",
    " .option(\"nullValue\", \"\") // Replace any null data with quotes\n",
    " .load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00f9ef",
   "metadata": {},
   "source": [
    "### Reading a CSV file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ac041",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e08a1",
   "metadata": {},
   "source": [
    "### Writing DataFrames to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac23bd",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfde64e",
   "metadata": {},
   "source": [
    "### Reading an Avro file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"avro\")\n",
    ".load(\"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/avro/*\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e10c3a",
   "metadata": {},
   "source": [
    "### Reading an Avro file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bde8a",
   "metadata": {},
   "source": [
    "### Writing DataFrames to Avro files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\n",
    " .format(\"avro\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"/tmp/data/avro/df_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae29c4",
   "metadata": {},
   "source": [
    "## ORC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf436409",
   "metadata": {},
   "source": [
    "### Reading an ORC file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbff3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/summary-data/orc/*\"\n",
    "val df = spark.read.format(\"orc\").load(file)\n",
    "df.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8b658",
   "metadata": {},
   "source": [
    "### Reading an ORC file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4082a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7afb38",
   "metadata": {},
   "source": [
    "### Writing DataFrames to ORC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11edb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"orc\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/orc/df_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f25ab",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725bd54",
   "metadata": {},
   "source": [
    "### Reading an image file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.source.image\n",
    "val imageDir = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/train_images/\"\n",
    "val imagesDF = spark.read.format(\"image\").load(imageDir)\n",
    "imagesDF.printSchema\n",
    "imagesDF.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    " \"label\").show(5, false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd6aac",
   "metadata": {},
   "source": [
    "## Binary Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24509e90",
   "metadata": {},
   "source": [
    "### Reading a binary file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val path = \"C:/Users/sara.arribas/Downloads/Ejemplos_Spark/train_images/\"\n",
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .load(path)\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .option(\"recursiveFileLookup\", \"true\")\n",
    " .load(path)\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b23825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
